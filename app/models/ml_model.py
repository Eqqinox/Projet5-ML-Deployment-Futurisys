"""
Wrapper pour le modÃ¨le XGBoost de prÃ©diction d'attrition
Gestion du chargement, des prÃ©dictions et de l'explicabilitÃ©
IMPORTANT: AdaptÃ© Ã  l'encodage OneHot du Projet 4
"""

import joblib
import numpy as np
import pandas as pd
from typing import List, Dict, Tuple, Optional
import os
from pathlib import Path
import logging
from datetime import datetime
from sklearn.preprocessing import OneHotEncoder, OrdinalEncoder

from app.models.schemas import EmployeeData, PredictionResult
from app.core.config import settings

logger = logging.getLogger(__name__)

class MLModel:
    """
    Wrapper pour le modÃ¨le XGBoost de prÃ©diction d'attrition des employÃ©s
    AdaptÃ© Ã  l'encodage OneHot utilisÃ© dans le Projet 4
    """
    
    def __init__(self):
        self.model = None
        self.model_version = settings.MODEL_VERSION
        self.threshold = 0.514  # Seuil optimal de votre Projet 4
        
        # Encodeurs (chargÃ©s depuis les fichiers sauvegardÃ©s du Projet 4)
        self.onehot_encoder = None
        self.ordinal_encoder = None
        self.final_column_names = None
        self.model_info = None
        
        # Variables pour l'encodage
        self.variables_catego = ['departement', 'domaine_etude', 'poste', 'statut_marital']
        self.is_loaded = False
        
    def _load_encoders(self):
        """
        Charge les encodeurs sauvegardÃ©s depuis le Projet 4
        """
        try:
            models_dir = Path(settings.MODEL_PATH).parent
            
            # Chargement des encodeurs
            onehot_path = models_dir / 'onehot_encoder.pkl'
            ordinal_path = models_dir / 'ordinal_encoder.pkl'
            columns_path = models_dir / 'final_column_names.pkl'
            info_path = models_dir / 'model_info.pkl'
            
            if not all([onehot_path.exists(), ordinal_path.exists(), columns_path.exists()]):
                raise FileNotFoundError(
                    f"Encodeurs manquants dans {models_dir}. "
                    f"ExÃ©cutez d'abord le script de sauvegarde dans le Projet 4."
                )
            
            self.onehot_encoder = joblib.load(onehot_path)
            self.ordinal_encoder = joblib.load(ordinal_path)
            self.final_column_names = joblib.load(columns_path)
            
            if info_path.exists():
                self.model_info = joblib.load(info_path)
            
            logger.info(f"âœ… Encodeurs chargÃ©s avec succÃ¨s")
            logger.info(f"ðŸ“Š Nombre de features finales: {len(self.final_column_names)}")
            
            return True
            
        except Exception as e:
            logger.error(f"âŒ Erreur lors du chargement des encodeurs: {e}")
            raise e
        
    def load_model(self) -> bool:
        """
        Charge le modÃ¨le XGBoost et les encodeurs depuis les fichiers .pkl
        """
        try:
            model_path = Path(settings.MODEL_PATH)
            
            if not model_path.exists():
                raise FileNotFoundError(f"ModÃ¨le non trouvÃ©: {model_path}")
            
            # Chargement du modÃ¨le XGBoost
            self.model = joblib.load(model_path)
            
            # Chargement des encodeurs
            self._load_encoders()
            
            self.is_loaded = True
            
            logger.info(f"âœ… ModÃ¨le chargÃ© avec succÃ¨s depuis {model_path}")
            logger.info(f"ðŸ“Š Type de modÃ¨le: {type(self.model)}")
            
            return True
            
        except Exception as e:
            logger.error(f"âŒ Erreur lors du chargement du modÃ¨le: {e}")
            raise e
    
    def _preprocess_employee_data(self, employee: EmployeeData) -> np.ndarray:
        """
        PrÃ©processe les donnÃ©es d'un employÃ© selon l'encodage du Projet 4
        """
        try:
            # Conversion en dictionnaire
            data_dict = employee.dict()
            
            # CrÃ©er un DataFrame avec une seule ligne
            df = pd.DataFrame([data_dict])
            
            # 1. Encodage binaire pour heure_supplementaires
            df.loc[df['heure_supplementaires'] == 'Non', 'heure_supplementaires'] = 0
            df.loc[df['heure_supplementaires'] == 'Oui', 'heure_supplementaires'] = 1
            df['heure_supplementaires'] = df['heure_supplementaires'].astype(int)
            
            # 2. Encodage binaire pour genre (adapter selon vos donnÃ©es d'entrÃ©e)
            # Note: Votre schÃ©ma Pydantic utilise "Homme"/"Femme" mais votre code "M"/"F"
            # Adaptation nÃ©cessaire:
            df.loc[df['genre'] == 'Femme', 'genre'] = 0
            df.loc[df['genre'] == 'Homme', 'genre'] = 1
            df['genre'] = df['genre'].astype(int)
            
            # 3. augementation_salaire_precedente reste float (c'est dÃ©jÃ  un pourcentage)
            # Pas de transformation nÃ©cessaire
            
            # 4. Encodage ordinal pour frequence_deplacement
            # Adapter les valeurs du schÃ©ma vers vos valeurs d'entraÃ®nement
            freq_mapping = {
                'Pas_de_Voyage': 'Aucun',
                'Voyage_Rare': 'Occasionnel', 
                'Voyage_FrÃ©quent': 'Frequent'
            }
            df['frequence_deplacement'] = df['frequence_deplacement'].map(freq_mapping)
            df[['frequence_deplacement']] = self.ordinal_encoder.transform(df[['frequence_deplacement']])
            
            # 5. Encodage OneHot pour les variables nominales
            df_categorical = df[self.variables_catego]
            df_encoded = self.onehot_encoder.transform(df_categorical)
            
            # CrÃ©er DataFrame avec les nouvelles colonnes
            feature_names = self.onehot_encoder.get_feature_names_out(self.variables_catego)
            df_onehot = pd.DataFrame(df_encoded, columns=feature_names, index=df.index)
            
            # Supprimer les colonnes originales et concatener les encodÃ©es
            df = df.drop(self.variables_catego, axis=1)
            df = pd.concat([df, df_onehot], axis=1)
            
            # Supprimer la variable cible si elle existe
            if 'a_quitte_l_entreprise' in df.columns:
                df = df.drop('a_quitte_l_entreprise', axis=1)
            
            # RÃ©organiser les colonnes selon l'ordre d'entraÃ®nement
            if self.final_column_names:
                # S'assurer que toutes les colonnes attendues sont prÃ©sentes
                missing_cols = set(self.final_column_names) - set(df.columns)
                if missing_cols:
                    logger.error(f"Colonnes manquantes aprÃ¨s encodage: {missing_cols}")
                    raise ValueError(f"Colonnes manquantes: {missing_cols}")
                
                # RÃ©organiser dans le bon ordre
                df = df[self.final_column_names]
            
            # Conversion en array NumPy
            return df.values
            
        except Exception as e:
            logger.error(f"Erreur lors du prÃ©processing: {e}")
            logger.error(f"Colonnes disponibles: {df.columns.tolist() if 'df' in locals() else 'DataFrame non crÃ©Ã©'}")
            if hasattr(self, 'final_column_names') and self.final_column_names:
                logger.error(f"Colonnes attendues: {self.final_column_names[:10]}...")
            raise e
    
    def predict_single(self, employee: EmployeeData, employee_id: Optional[str] = None) -> PredictionResult:
        """
        Effectue une prÃ©diction pour un seul employÃ©
        """
        if not self.is_loaded:
            raise RuntimeError("Le modÃ¨le n'est pas chargÃ©")
        
        try:
            # PrÃ©processing
            X = self._preprocess_employee_data(employee)
            
            # PrÃ©diction des probabilitÃ©s
            probabilities = self.model.predict_proba(X)[0]
            prob_stay = probabilities[0]  # ProbabilitÃ© de rester (classe 0)
            prob_quit = probabilities[1]  # ProbabilitÃ© de partir (classe 1)
            
            # DÃ©cision basÃ©e sur le seuil
            prediction = "Oui" if prob_quit >= self.threshold else "Non"
            
            # Niveau de confiance
            confidence_level = self._get_confidence_level(max(prob_stay, prob_quit))
            
            # Facteurs de risque (basique, peut Ãªtre amÃ©liorÃ© avec SHAP)
            risk_factors = self._identify_risk_factors(employee)
            
            return PredictionResult(
                employee_id=employee_id,
                prediction=prediction,
                probability_quit=round(prob_quit, 4),
                probability_stay=round(prob_stay, 4),
                confidence_level=confidence_level,
                risk_factors=risk_factors,
                model_version=self.model_version,
                timestamp=datetime.now()
            )
            
        except Exception as e:
            logger.error(f"Erreur lors de la prÃ©diction: {e}")
            raise e
    
    def predict_batch(self, employees: List[EmployeeData]) -> List[PredictionResult]:
        """
        Effectue des prÃ©dictions pour plusieurs employÃ©s
        """
        if not self.is_loaded:
            raise RuntimeError("Le modÃ¨le n'est pas chargÃ©")
        
        predictions = []
        for i, employee in enumerate(employees):
            try:
                prediction = self.predict_single(employee, employee_id=f"emp_{i+1}")
                predictions.append(prediction)
            except Exception as e:
                logger.error(f"Erreur pour l'employÃ© {i+1}: {e}")
                # Continuer avec les autres employÃ©s
                continue
        
        return predictions
    
    def _get_confidence_level(self, max_probability: float) -> str:
        """
        DÃ©termine le niveau de confiance basÃ© sur la probabilitÃ© maximale
        """
        if max_probability >= 0.8:
            return "Ã‰levÃ©"
        elif max_probability >= 0.6:
            return "Moyen"
        else:
            return "Faible"
    
    def _identify_risk_factors(self, employee: EmployeeData) -> List[str]:
        """
        Identifie les facteurs de risque basiques
        Cette fonction peut Ãªtre amÃ©liorÃ©e avec SHAP values
        """
        risk_factors = []
        
        # RÃ¨gles heuristiques basiques
        if employee.satisfaction_employee_environnement <= 3:
            risk_factors.append("Satisfaction environnement trÃ¨s faible")
        
        if employee.satisfaction_employee_nature_travail <= 3:
            risk_factors.append("Satisfaction travail trÃ¨s faible")
        
        if employee.heure_supplementaires == "Oui":
            risk_factors.append("Heures supplÃ©mentaires frÃ©quentes")
        
        if employee.augementation_salaire_precedente == "Non":
            risk_factors.append("Pas d'augmentation rÃ©cente")
        
        if employee.annees_depuis_la_derniere_promotion >= 5:
            risk_factors.append("Pas de promotion depuis longtemps")
        
        if employee.distance_domicile_travail >= 30:
            risk_factors.append("Distance domicile-travail importante")
        
        return risk_factors[:5]  # Limiter Ã  5 facteurs principaux
    
    def get_model_info(self) -> Dict:
        """
        Retourne les informations sur le modÃ¨le
        """
        
        base_info = {
            "model_name": "XGBoost Employee Attrition Classifier",
            "model_type": "XGBoost Classifier", 
            "version": self.model_version,
            "threshold": self.threshold,
            "is_loaded": self.is_loaded,
            "encoding": "OneHot pour variables nominales, Ordinal pour frÃ©quence",
        }
        
        # Ajouter les informations du Projet 4 si disponibles
        if self.model_info:
            base_info.update({
                "original_features": self.model_info.get('original_features_count', 27),
                "encoded_features": self.model_info.get('final_features_count', 'Unknown'),
                "categorical_variables_encoded": self.model_info.get('categorical_variables_encoded', []),
            })
        elif self.final_column_names:
            base_info.update({
                "original_features": 27,
                "encoded_features": len(self.final_column_names),
                "categorical_variables_encoded": self.variables_catego,
            })
        
        # MÃ©triques rÃ©elles du modÃ¨le avec validation croisÃ©e stratifiÃ©e
        base_info["performance_metrics"] = {
            "accuracy": 0.8588,          # CV moyenne
            "accuracy_std": 0.0220,      # Ã‰cart-type CV
            "precision": 0.5654,         # CV moyenne  
            "precision_std": 0.0701,     # Ã‰cart-type CV
            "recall": 0.5684,            # CV moyenne
            "recall_std": 0.0678,        # Ã‰cart-type CV
            "f1_score": 0.5656,          # CV moyenne
            "f1_std": 0.0638,            # Ã‰cart-type CV
            "roc_auc": 0.8252,           # CV moyenne
            "roc_auc_std": 0.0212        # Ã‰cart-type CV
        }
        
        return base_info
    
    def health_check(self) -> Dict[str, bool]:
        """
        VÃ©rifie la santÃ© du modÃ¨le
        """
        return {
            "model_loaded": self.is_loaded,
            "model_file_exists": os.path.exists(settings.MODEL_PATH),
            "encoders_loaded": self.onehot_encoder is not None and self.ordinal_encoder is not None,
            "column_names_loaded": self.final_column_names is not None,
            "all_encoder_files_exist": self._check_encoder_files_exist()
        }
        
    def _check_encoder_files_exist(self) -> bool:
        """VÃ©rifie que tous les fichiers d'encodeurs existent"""
        try:
            models_dir = Path(settings.MODEL_PATH).parent
            required_files = [
                'onehot_encoder.pkl',
                'ordinal_encoder.pkl', 
                'final_column_names.pkl'
            ]
            return all((models_dir / f).exists() for f in required_files)
        except:
            return False