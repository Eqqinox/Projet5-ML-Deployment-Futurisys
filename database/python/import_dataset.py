#!/usr/bin/env python3
"""
Script d'import du dataset du Projet 4 dans PostgreSQL - VERSION AUTONOME
Import des 1470 employ√©s avec leurs 27 features + variable cible
"""

import os
import sys
import pandas as pd
from pathlib import Path
from typing import Optional, Dict, List
import logging
from datetime import datetime
from dotenv import load_dotenv

from sqlalchemy import (
    create_engine, Column, Integer, String, Boolean, DateTime, 
    Numeric, Text, ForeignKey, func
)
from sqlalchemy.ext.declarative import declarative_base
from sqlalchemy.orm import sessionmaker, relationship
from sqlalchemy.dialects.postgresql import UUID, JSONB, INET, ARRAY
import uuid

# Charger les variables d'environnement
load_dotenv()

# Configuration simple
class Settings:
    DATABASE_URL = os.getenv('DATABASE_URL')
    POSTGRES_HOST = os.getenv('POSTGRES_HOST', 'localhost')
    POSTGRES_PORT = int(os.getenv('POSTGRES_PORT', 5432))
    POSTGRES_USER = os.getenv('POSTGRES_USER')
    POSTGRES_PASSWORD = os.getenv('POSTGRES_PASSWORD')
    POSTGRES_DB = os.getenv('POSTGRES_DB')

settings = Settings()

# Base SQLAlchemy (reproduction du mod√®le Employee)
Base = declarative_base()

class Employee(Base):
    """Mod√®le Employee pour l'import"""
    __tablename__ = 'employees'
    
    employee_id = Column(Integer, primary_key=True, autoincrement=True)
    satisfaction_employee_environnement = Column(Integer, nullable=False)
    satisfaction_employee_nature_travail = Column(Integer, nullable=False)
    satisfaction_employee_equipe = Column(Integer, nullable=False)
    satisfaction_employee_equilibre_pro_perso = Column(Integer, nullable=False)
    note_evaluation_precedente = Column(Integer, nullable=False)
    note_evaluation_actuelle = Column(Integer, nullable=False)
    niveau_hierarchique_poste = Column(Integer, nullable=False)
    heure_supplementaires = Column(String(5), nullable=False)
    augementation_salaire_precedente = Column(Numeric(6,4), nullable=False)
    age = Column(Integer, nullable=False)
    genre = Column(String(5), nullable=False)
    revenu_mensuel = Column(Integer, nullable=False)
    statut_marital = Column(String(20), nullable=False)
    departement = Column(String(30), nullable=False)
    poste = Column(String(50), nullable=False)
    nombre_experiences_precedentes = Column(Integer, nullable=False)
    annee_experience_totale = Column(Integer, nullable=False)
    annees_dans_l_entreprise = Column(Integer, nullable=False)
    annees_dans_le_poste_actuel = Column(Integer, nullable=False)
    annees_depuis_la_derniere_promotion = Column(Integer, nullable=False)
    annes_sous_responsable_actuel = Column(Integer, nullable=False)
    nombre_participation_pee = Column(Integer, nullable=False)
    nb_formations_suivies = Column(Integer, nullable=False)
    distance_domicile_travail = Column(Integer, nullable=False)
    niveau_education = Column(Integer, nullable=False)
    domaine_etude = Column(String(50), nullable=False)
    frequence_deplacement = Column(String(20), nullable=False)
    a_quitte_l_entreprise = Column(String(5), nullable=False)
    created_at = Column(DateTime, default=func.current_timestamp())
    updated_at = Column(DateTime, default=func.current_timestamp(), onupdate=func.current_timestamp())

# Configuration du logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)

def create_database_url():
    """Cr√©e l'URL de connexion √† la base de donn√©es"""
    return settings.DATABASE_URL or f"postgresql://{settings.POSTGRES_USER}:{settings.POSTGRES_PASSWORD}@{settings.POSTGRES_HOST}:{settings.POSTGRES_PORT}/{settings.POSTGRES_DB}"

def create_engine_with_settings(database_url: str = None, echo: bool = False):
    """Cr√©e le moteur SQLAlchemy"""
    if not database_url:
        database_url = create_database_url()
    
    return create_engine(
        database_url,
        echo=echo,
        pool_size=5,
        max_overflow=10,
        pool_timeout=30,
        pool_recycle=3600,
        pool_pre_ping=True
    )

class DatasetImporter:
    """Classe pour importer le dataset du Projet 4 dans PostgreSQL"""
    
    def __init__(self, database_url: str = None):
        self.engine = create_engine_with_settings(database_url)
        self.SessionLocal = sessionmaker(bind=self.engine)
        self.stats = {
            'total_rows': 0,
            'imported_rows': 0,
            'errors': 0,
            'skipped_rows': 0,
            'start_time': None,
            'end_time': None
        }
    
    def validate_csv_structure(self, df: pd.DataFrame) -> bool:
        """Valide la structure du CSV par rapport au mod√®le Employee"""
        logger.info("üîç Validation de la structure du dataset...")
        
        # Colonnes attendues (27 features + target)
        expected_columns = [
            'satisfaction_employee_environnement',
            'note_evaluation_precedente', 
            'niveau_hierarchique_poste',
            'satisfaction_employee_nature_travail',
            'satisfaction_employee_equipe',
            'satisfaction_employee_equilibre_pro_perso',
            'note_evaluation_actuelle',
            'heure_supplementaires',
            'augementation_salaire_precedente',
            'age',
            'genre',
            'revenu_mensuel',
            'statut_marital',
            'departement',
            'poste',
            'nombre_experiences_precedentes',
            'annee_experience_totale',
            'annees_dans_l_entreprise',
            'annees_dans_le_poste_actuel',
            'a_quitte_l_entreprise',
            'nombre_participation_pee',
            'nb_formations_suivies',
            'distance_domicile_travail',
            'niveau_education',
            'domaine_etude',
            'frequence_deplacement',
            'annees_depuis_la_derniere_promotion',
            'annes_sous_responsable_actuel'
        ]
        
        # V√©rification des colonnes
        missing_columns = set(expected_columns) - set(df.columns)
        extra_columns = set(df.columns) - set(expected_columns)
        
        if missing_columns:
            logger.error(f"‚ùå Colonnes manquantes: {missing_columns}")
            return False
            
        if extra_columns:
            logger.warning(f"‚ö†Ô∏è Colonnes suppl√©mentaires ignor√©es: {extra_columns}")
        
        # V√©rification du nombre de lignes
        if len(df) != 1470:
            logger.warning(f"‚ö†Ô∏è Nombre de lignes attendu: 1470, trouv√©: {len(df)}")
        
        # V√©rification des valeurs manquantes
        null_counts = df[expected_columns].isnull().sum()
        if null_counts.sum() > 0:
            logger.error("‚ùå Valeurs manquantes d√©tect√©es:")
            for col, count in null_counts[null_counts > 0].items():
                logger.error(f"  {col}: {count} valeurs manquantes")
            return False
        
        logger.info("‚úÖ Structure du dataset valid√©e")
        return True
    
    def validate_data_types_and_ranges(self, df: pd.DataFrame) -> pd.DataFrame:
        """Valide et nettoie les types de donn√©es et les plages de valeurs"""
        logger.info("üßπ Validation et nettoyage des donn√©es...")
        
        df_clean = df.copy()
        validation_errors = []
        
        # Variables de satisfaction (1-4)
        satisfaction_cols = [
            'satisfaction_employee_environnement',
            'satisfaction_employee_nature_travail', 
            'satisfaction_employee_equipe',
            'satisfaction_employee_equilibre_pro_perso'
        ]
        
        for col in satisfaction_cols:
            mask = (df_clean[col] < 1) | (df_clean[col] > 4)
            if mask.any():
                validation_errors.append(f"{col}: {mask.sum()} valeurs hors plage [1-4]")
        
        # Variables d'√©valuation (1-4)
        evaluation_cols = ['note_evaluation_precedente', 'note_evaluation_actuelle']
        for col in evaluation_cols:
            mask = (df_clean[col] < 1) | (df_clean[col] > 4)
            if mask.any():
                validation_errors.append(f"{col}: {mask.sum()} valeurs hors plage [1-4]")
        
        # Niveau hi√©rarchique (1-5)
        mask = (df_clean['niveau_hierarchique_poste'] < 1) | (df_clean['niveau_hierarchique_poste'] > 5)
        if mask.any():
            validation_errors.append(f"niveau_hierarchique_poste: {mask.sum()} valeurs hors plage [1-5]")
        
        # √Çge (18-60)
        mask = (df_clean['age'] < 18) | (df_clean['age'] > 60)
        if mask.any():
            validation_errors.append(f"age: {mask.sum()} valeurs hors plage [18-60]")
        
        # Revenu mensuel (1000-20000)
        mask = (df_clean['revenu_mensuel'] < 1000) | (df_clean['revenu_mensuel'] > 20000)
        if mask.any():
            validation_errors.append(f"revenu_mensuel: {mask.sum()} valeurs hors plage [1000-20000]")
        
        # Variables cat√©gorielles - validation des valeurs
        categorical_validations = {
            'heure_supplementaires': ['Oui', 'Non'],
            'genre': ['F', 'M'],
            'statut_marital': ['C√©libataire', 'Mari√©(e)', 'Divorc√©(e)'],
            'departement': ['Commercial', 'Consulting', 'Ressources Humaines'],
            'poste': [
                'Cadre Commercial', 'Assistant de Direction', 'Consultant',
                'Tech Lead', 'Manager', 'Senior Manager', 'Repr√©sentant Commercial',
                'Directeur Technique', 'Ressources Humaines'
            ],
            'domaine_etude': [
                'Infra & Cloud', 'Autre', 'Transformation Digitale',
                'Marketing', 'Entrepreunariat', 'Ressources Humaines'
            ],
            'frequence_deplacement': ['Aucun', 'Occasionnel', 'Frequent'],
            'a_quitte_l_entreprise': ['Oui', 'Non']
        }
        
        for col, valid_values in categorical_validations.items():
            invalid_mask = ~df_clean[col].isin(valid_values)
            if invalid_mask.any():
                invalid_values = df_clean[invalid_mask][col].unique()
                validation_errors.append(f"{col}: valeurs invalides {list(invalid_values)}")
        
        # Variables de participation PEE (0-3)
        mask = (df_clean['nombre_participation_pee'] < 0) | (df_clean['nombre_participation_pee'] > 3)
        if mask.any():
            validation_errors.append(f"nombre_participation_pee: {mask.sum()} valeurs hors plage [0-3]")
        
        # Formations suivies (0-6)
        mask = (df_clean['nb_formations_suivies'] < 0) | (df_clean['nb_formations_suivies'] > 6)
        if mask.any():
            validation_errors.append(f"nb_formations_suivies: {mask.sum()} valeurs hors plage [0-6]")
        
        # Distance domicile-travail (1-29)
        mask = (df_clean['distance_domicile_travail'] < 1) | (df_clean['distance_domicile_travail'] > 29)
        if mask.any():
            validation_errors.append(f"distance_domicile_travail: {mask.sum()} valeurs hors plage [1-29]")
        
        # Niveau √©ducation (1-5)
        mask = (df_clean['niveau_education'] < 1) | (df_clean['niveau_education'] > 5)
        if mask.any():
            validation_errors.append(f"niveau_education: {mask.sum()} valeurs hors plage [1-5]")
        
        if validation_errors:
            logger.warning("‚ö†Ô∏è Erreurs de validation d√©tect√©es (affichage seulement):")
            for error in validation_errors:
                logger.warning(f"  {error}")
            # Ne pas lever d'erreur, juste afficher les warnings
        
        logger.info("‚úÖ Validation des donn√©es termin√©e")
        return df_clean
    
    def prepare_data_for_import(self, df: pd.DataFrame) -> List[Dict]:
        """Pr√©pare les donn√©es pour l'insertion en base"""
        logger.info("üîß Pr√©paration des donn√©es pour l'import...")
        
        records = []
        for _, row in df.iterrows():
            record = row.to_dict()
            record['created_at'] = datetime.now()
            record['updated_at'] = datetime.now()
            records.append(record)
        
        logger.info(f"‚úÖ {len(records)} enregistrements pr√©par√©s")
        return records
    
    def clear_existing_data(self, session, confirm: bool = False) -> bool:
        """Supprime les donn√©es existantes"""
        existing_count = session.query(Employee).count()
        
        if existing_count == 0:
            logger.info("üìù Aucune donn√©e existante √† supprimer")
            return True
        
        logger.warning(f"‚ö†Ô∏è {existing_count} employ√©s trouv√©s dans la base")
        
        if not confirm:
            response = input("‚ùì Supprimer les donn√©es existantes ? (yes/no): ")
            if response.lower() not in ['yes', 'y']:
                logger.info("‚ùå Import annul√© par l'utilisateur")
                return False
        
        try:
            deleted_count = session.query(Employee).delete()
            session.commit()
            logger.info(f"üóëÔ∏è {deleted_count} enregistrements supprim√©s")
            return True
        except Exception as e:
            logger.error(f"‚ùå Erreur lors de la suppression: {e}")
            session.rollback()
            return False
    
    def import_data_batch(self, session, records: List[Dict], batch_size: int = 100):
        """Importe les donn√©es par batch"""
        logger.info(f"üì• Import de {len(records)} enregistrements par batch de {batch_size}...")
        
        imported_count = 0
        error_count = 0
        
        for i in range(0, len(records), batch_size):
            batch = records[i:i + batch_size]
            
            try:
                employees = [Employee(**record) for record in batch]
                session.add_all(employees)
                session.commit()
                
                imported_count += len(batch)
                logger.info(f"‚úÖ Batch {i//batch_size + 1}: {len(batch)} enregistrements import√©s")
                
            except Exception as e:
                logger.error(f"‚ùå Erreur batch {i//batch_size + 1}: {e}")
                session.rollback()
                error_count += len(batch)
                
                # Tentative d'import ligne par ligne
                for j, record in enumerate(batch):
                    try:
                        employee = Employee(**record)
                        session.add(employee)
                        session.commit()
                        imported_count += 1
                    except Exception as detail_error:
                        logger.error(f"‚ùå Erreur ligne {i+j+1}: {detail_error}")
                        session.rollback()
        
        self.stats['imported_rows'] = imported_count
        self.stats['errors'] = error_count
        
        logger.info(f"üìä Import termin√©: {imported_count} r√©ussites, {error_count} erreurs")
        return imported_count, error_count
    
    def verify_import(self, session):
        """V√©rifie l'int√©grit√© des donn√©es import√©es"""
        logger.info("üîç V√©rification de l'import...")
        
        total_count = session.query(Employee).count()
        logger.info(f"üìä Total employ√©s en base: {total_count}")
        
        # Statistiques par d√©partement
        dept_stats = session.query(
            Employee.departement,
            func.count(Employee.employee_id).label('count'),
            func.count(func.nullif(Employee.a_quitte_l_entreprise, 'Non')).label('quit_count')
        ).group_by(Employee.departement).all()
        
        logger.info("üìà Statistiques par d√©partement:")
        for dept, count, quit_count in dept_stats:
            quit_rate = (quit_count / count * 100) if count > 0 else 0
            logger.info(f"  {dept}: {count} employ√©s, {quit_count} d√©missions ({quit_rate:.1f}%)")
        
        logger.info("‚úÖ V√©rification termin√©e")
        return True
    
    def import_dataset(self, csv_file_path: str, clear_existing: bool = False, 
                      batch_size: int = 100, confirm_clear: bool = False) -> Dict:
        """Fonction principale d'import du dataset"""
        self.stats['start_time'] = datetime.now()
        
        try:
            logger.info("üöÄ D√©but de l'import du dataset Projet 4")
            logger.info("=" * 60)
            
            # Chargement du CSV
            logger.info(f"üìñ Chargement du fichier: {csv_file_path}")
            
            if not os.path.exists(csv_file_path):
                raise FileNotFoundError(f"Fichier CSV non trouv√©: {csv_file_path}")
            
            df = pd.read_csv(csv_file_path)
            self.stats['total_rows'] = len(df)
            
            logger.info(f"üìä Donn√©es charg√©es: {len(df)} lignes, {len(df.columns)} colonnes")
            
            # Validation de la structure
            if not self.validate_csv_structure(df):
                raise ValueError("Structure du CSV invalide")
            
            # Validation et nettoyage des donn√©es
            df_clean = self.validate_data_types_and_ranges(df)
            
            # Pr√©paration des donn√©es
            records = self.prepare_data_for_import(df_clean)
            
            # Connexion √† la base
            session = self.SessionLocal()
            
            try:
                # Suppression des donn√©es existantes (si demand√©)
                if clear_existing:
                    if not self.clear_existing_data(session, confirm_clear):
                        raise Exception("Suppression des donn√©es annul√©e")
                
                # Import des donn√©es
                imported_count, error_count = self.import_data_batch(session, records, batch_size)
                
                # V√©rification de l'import
                self.verify_import(session)
                
                self.stats['end_time'] = datetime.now()
                duration = (self.stats['end_time'] - self.stats['start_time']).total_seconds()
                
                logger.info("=" * 60)
                logger.info("‚úÖ Import termin√© avec succ√®s !")
                logger.info(f"üìä {imported_count}/{self.stats['total_rows']} enregistrements import√©s")
                logger.info(f"‚è±Ô∏è Dur√©e: {duration:.2f} secondes")
                
                return {
                    'success': True,
                    'imported_count': imported_count,
                    'total_count': self.stats['total_rows'],
                    'duration_seconds': duration
                }
                
            finally:
                session.close()
                
        except Exception as e:
            self.stats['end_time'] = datetime.now()
            logger.error(f"üí• Erreur lors de l'import: {e}")
            raise e

def find_dataset_file() -> Optional[str]:
    """Recherche automatique du fichier dataset"""
    possible_paths = [
        "data/raw/dataset_projet4.csv",
        "data/dataset_projet4.csv", 
        "dataset_projet4.csv",
        "../dataset_projet4.csv",
        "../../dataset_projet4.csv"
    ]
    
    for path in possible_paths:
        if os.path.exists(path):
            return path
    
    return None

def main():
    """Fonction principale"""
    import argparse
    
    parser = argparse.ArgumentParser(description="Import du dataset Projet 4 dans PostgreSQL")
    parser.add_argument("--file", type=str, help="Chemin vers le fichier CSV")
    parser.add_argument("--clear", action="store_true", help="Supprimer les donn√©es existantes")
    parser.add_argument("--batch-size", type=int, default=100, help="Taille des batches")
    parser.add_argument("--yes", action="store_true", help="Confirmer automatiquement")
    parser.add_argument("--url", type=str, help="URL de connexion personnalis√©e")
    
    args = parser.parse_args()
    
    try:
        # Recherche du fichier CSV
        csv_file = args.file
        if not csv_file:
            csv_file = find_dataset_file()
            if not csv_file:
                logger.error("‚ùå Fichier CSV non trouv√©. Utilisez --file pour sp√©cifier le chemin")
                sys.exit(1)
            else:
                logger.info(f"üìÅ Fichier trouv√© automatiquement: {csv_file}")
        
        # Import du dataset
        importer = DatasetImporter(database_url=args.url)
        
        result = importer.import_dataset(
            csv_file_path=csv_file,
            clear_existing=args.clear,
            batch_size=args.batch_size,
            confirm_clear=args.yes
        )
        
        logger.info("üéâ Script d'import termin√© avec succ√®s !")
        
    except Exception as e:
        logger.error(f"üí• √âchec de l'import: {e}")
        sys.exit(1)

if __name__ == "__main__":
    main()